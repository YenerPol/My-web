---
title: 'Part 1 - NLP (Data Science Capstone JHU / Coursera)'
author: Paul Gomez
date: '2021-05-29'
slug: part-1-data-science-capstone-nlp-jhu-coursera
categories:
  - R
tags:
  - Data Science
  - NLP
  - R Markdown
  - Exploratory Analysis
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>
<link href="{{< blogdown/postref >}}index.en_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index.en_files/anchor-sections/anchor-sections.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This series of post are dedicated to the final capstone of the <a href="https://www.coursera.org/specializations/jhu-data-science">Data science specialization</a> of the John Hopkins University through coursera platform. The goal of this capstone is to understand and building predictive text models. As time pass I will be posting my steps to complete each assigment, here called tasks, until achieve the goal of having a fully functional model and a shiny app to run it online.</p>
<p>This first post is dedicated to:</p>
<ul>
<li><strong>Task 1</strong> - Getting and cleaning the data</li>
<li><strong>Task 2</strong> - Exploratory Data Analysis</li>
</ul>
</div>
<div id="task-1---getting-and-cleaning-the-data" class="section level2">
<h2>Task 1 - Getting and Cleaning the Data</h2>
<p>In this part, I will be using an English database. Tasks to accomplish are:</p>
<ul>
<li><strong>Obtaining the data</strong></li>
<li><strong>Tokenization</strong> - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.</li>
<li><strong>Profanity filtering</strong> - removing profanity and other words I do not want to predict.</li>
</ul>
<div id="getting-the-data" class="section level3">
<h3>Getting the data</h3>
<p>The data was supplied for the instructors, but we are allowed to expand the data set if we consider it’s appropriate. As a first approach I will use just the data set supplied and depending of the results of this approach I may consider extend it in the future.</p>
<p>The data can be downloaded <a href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip">here</a>.</p>
<pre class="r"><code>library(tidyverse)

blogs &lt;- read_tsv(path$blogs) %&gt;% rename(&quot;text&quot;=&quot;sample_lines.x..n...round.nl.20..&quot;)
news &lt;- read_tsv(path$news) %&gt;% rename(&quot;text&quot;=&quot;sample_lines.x..n...round.nl.20..&quot;)
twitter &lt;- read_tsv(path$twitter) %&gt;% rename(&quot;text&quot;=&quot;sample_lines.x..n...round.nl.20..&quot;)
rm(path)</code></pre>
<p>A fast exploration of this files says blogs data has 44964 lines, News has 50512 lines and twitter 44964 lines.</p>
<p>Preview of blogs data set:</p>
<pre><code>## # A tibble: 6 x 1
##   text                                                                          
##   &lt;chr&gt;                                                                         
## 1 &quot;MC-BC, if you still donâ€™t get it.\r&quot;                                       
## 2 &quot;Monday morningâ€¦bright and earlyâ€¦well, not bright, but early. And itâ€™s ~
## 3 &quot;~It must be voted into the Watchlist by majority vote. If there is an unsett~
## 4 &quot;7. Donâ€™t use adverbs to strengthen a verb. Use adverbs to change the meani~
## 5 &quot;Simply Playing - Carries Waldorf inspired all natural plant dyed wool toys t~
## 6 &quot;eating Thai green curry\r&quot;</code></pre>
</div>
<div id="tokenization" class="section level3">
<h3>Tokenization</h3>
<p>Tokenization is a key (and mandatory) aspect of working with text data, Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.</p>
<p>Tokenization is performed on the corpus to obtain tokens. The following tokens are then used to prepare a vocabulary. Vocabulary refers to the set of unique tokens in the corpus.</p>
<p>To perform an exploratory analysis, I’ll tokenize the data in 1 word, 2 words and 3 words (1-gram, 2-grams, 3-grams). I will use the tidytext package for this task. First define a function to</p>
<pre class="r"><code>library(tidytext)

token_func &lt;- function(rawdata){
  # Input : raw data. tibble n x 1. 1 sentence per row. 
  # Output : tokenized Version of sentences. list of tibbles.
  #           list[[1]] is a tibble m x 1 (1-gram)
  #           list[[2]] is a tibble m x 2 (2-gram)
  #           list[[3]] is a tibble m x 3 (3-gram)
  token_w4 &lt;- rawdata %&gt;% 
    unnest_tokens(word, text, token = &quot;ngrams&quot;, n=3, to_lower = T)
  token_w4 &lt;- token_w4 %&gt;% separate(word, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;)
    tokens_list &lt;- list(token_w4[,1],token_w4[,1:2],token_w4[,1:3])
}

blogs_list &lt;- token_func(blogs)  
news_list &lt;- token_func(news)
twitter_list &lt;- token_func(twitter)

token1 &lt;- rbind(blogs_list[[1]], news_list[[1]], twitter_list[[1]])
token2 &lt;- rbind(blogs_list[[2]], news_list[[2]], twitter_list[[2]])
token3 &lt;- rbind(blogs_list[[3]], news_list[[3]], twitter_list[[3]])

head(token3)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   word1 word2 word3
##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
## 1 mc    bc    if   
## 2 bc    if    you  
## 3 if    you   still
## 4 you   still donâ 
## 5 still donâ  t    
## 6 donâ  t     get</code></pre>
</div>
<div id="profanity-filtering" class="section level3">
<h3>Profanity filtering</h3>
<p>In this step we want to remove profanity and other words we do not want to predict. I’ll use the <strong>lexicon</strong> package as source for a profanity vocabulary. First step is to create a tibble with profan vacabulary:</p>
<pre class="r"><code>library(lexicon) 
profan_words &lt;- tibble(profanity_alvarez) %&gt;% rename(&quot;word&quot;=&quot;profanity_alvarez&quot;)</code></pre>
<p>Now, we need a function to filter our data. In addition to profanity filytering, I’m going to make some aditional filtering. The function first filter one character words, words that contain numbers and some special characters, finally remove profanity words.</p>
<pre class="r"><code>separar_filtrar &lt;- function(data, profan){
    if(ncol(data) == 3){
        ngrams_filtered &lt;- data %&gt;%
            filter(!(is.na(word1) | is.na(word2) | is.na(word3) )) %&gt;% 
            filter(!((nchar(word1) == 1) | (nchar(word2) == 1) | (nchar(word3) == 1) ) ) %&gt;% 
            filter(!(str_detect(word1, &quot;[0-9]&quot;) | str_detect(word2, &quot;[0-9]&quot;) | str_detect(word3, &quot;[0-9]&quot;) ) ) %&gt;% 
            filter(!(str_detect(word1, &quot;â&quot;) | str_detect(word2, &quot;â&quot;) | str_detect(word3, &quot;â&quot;) ) ) %&gt;%
            filter(!(word1 %in% profan$word | word2 %in% profan$word | word3 %in% profan$word)) %&gt;% 
            filter(!(str_detect(word1, &quot;hah&quot;) | str_detect(word2, &quot;hah&quot;) | str_detect(word3, &quot;hah&quot;) ) )
    }else if(ncol(data) == 2){
        ngrams_filtered &lt;- data %&gt;%
            filter(!(is.na(word1) | is.na(word2) )) %&gt;% 
            filter(!((nchar(word1) == 1) | (nchar(word2) == 1) ) ) %&gt;% 
            filter(!(str_detect(word1, &quot;[0-9]&quot;) | str_detect(word2, &quot;[0-9]&quot;) ) ) %&gt;% 
            filter(!(str_detect(word1, &quot;â&quot;) | str_detect(word2, &quot;â&quot;) ) ) %&gt;% 
            filter(!(word1 %in% profan$word | word2 %in% profan$word )) %&gt;%
            filter(!(str_detect(word1, &quot;hah&quot;) | str_detect(word2, &quot;hah&quot;) ) )            
    }else if(ncol(data) == 1){
        ngrams_filtered &lt;- data %&gt;%
            filter(!(is.na(word1))) %&gt;% 
            filter(!((nchar(word1) == 1) ) ) %&gt;% 
            filter(!(str_detect(word1, &quot;[0-9]&quot;) )) %&gt;% 
            filter(!(str_detect(word1, &quot;â&quot;) ) ) %&gt;% 
            filter(!(word1 %in% profan$word )) %&gt;%
            filter(!(str_detect(word1, &quot;hah&quot;) ) )
    }
        ngrams_filtered
}</code></pre>
<p>Applying the <em>separar_filtrar()</em> function to out data:</p>
<pre class="r"><code>token1_task_1 &lt;- separar_filtrar(token1, profan_words)
token2_task_1 &lt;- separar_filtrar(token2, profan_words)
token3_task_1 &lt;- separar_filtrar(token3, profan_words)</code></pre>
</div>
</div>
<div id="task-2---exploratory-data-analysis" class="section level2">
<h2>Task 2 - Exploratory Data Analysis</h2>
<p>A good first step to understand the vocabulary is to count frequency of tokens. Lets begin exploring the 1-word per token (1-gram) data:</p>
<pre class="r"><code>token1_task_1 %&gt;% count(word1, sort = TRUE) %&gt;% summary()</code></pre>
<pre><code>##     word1                 n            
##  Length:110663      Min.   :     1.00  
##  Class :character   1st Qu.:     1.00  
##  Mode  :character   Median :     2.00  
##                     Mean   :    39.53  
##                     3rd Qu.:     5.00  
##                     Max.   :225943.00</code></pre>
<p>Here we can see that we have a really huge vocabulary, there are more than a hundred thousand words! But, in the other hand, the summary of this vocabulary shows that 75% of the words are repeated just 5 times or less. Cut the data by frecuency could help making our model faster and lighther. The most frequent words are:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/frequency%20plot%201%20gram-1.png" width="672" style="display: block; margin: auto;" />
Now I will select just words with a frequency &gt;3, we want to reduce the vocabulary size:</p>
<pre class="r"><code># selecting words with count &gt;3
token1 &lt;- token1_task_1 %&gt;% 
  count(word1, sort = T) %&gt;% 
  filter(n &gt; 3) %&gt;% 
  select(word1)

head(token1)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   word1
##   &lt;chr&gt;
## 1 the  
## 2 to   
## 3 and  
## 4 of   
## 5 in   
## 6 for</code></pre>
<p>Now we have a vocabulary of 50512 words. The next step is to filter the 2-gram and 3-grams tokens to keep just words which belongs to the vocabulary:</p>
<pre class="r"><code># this function gets 2-gram and 3-gram data (1 word per column) and 
# remove words that doesn&#39;t appear in the vocabulary 
clean_func &lt;- function(dat, vocabulary){
    if(ncol(dat) == 2){
        dat %&gt;% filter((word1 %in% vocabulary$word1) | (word2 %in% vocabulary$word1))
    }else if(ncol(dat) == 3){
        dat %&gt;% filter((word1 %in% vocabulary$word1) | (word2 %in% vocabulary$word1)  | (word3 %in% vocabulary$word1))
    }
}

# Filtering token2 y token3
token2 &lt;- clean_func(token2_task_1, token1)
token3 &lt;- clean_func(token3_task_1, token1)
rm(token1_task_1, token2_task_1, token3_task_1, blogs, news, twitter)

head(token3)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   word1  word2  word3 
##   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; 
## 1 mc     bc     if    
## 2 bc     if     you   
## 3 if     you    still 
## 4 well   not    bright
## 5 not    bright but   
## 6 bright but    early</code></pre>
<p>To count Bigrams and Trigrams, it’s necesesary to combine each row in a single character column:</p>
<pre class="r"><code>token2 &lt;- token2 %&gt;% 
    unite(bigram, word1, word2, sep = &quot; &quot;) %&gt;% 
    count(bigram, sort = T)

token3 &lt;- token3 %&gt;% 
    unite(trigram, word1, word2, word3, sep = &quot; &quot;) %&gt;% 
    count(trigram, sort = T)</code></pre>
<p>Now we can see which are the most common bigrams and trigrams in our data:
<img src="{{< blogdown/postref >}}index.en_files/figure-html/frequency%20plot%202%20gram-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/frequency%20plot%203%20gram-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
