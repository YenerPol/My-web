---
title: 'Part 1 - NLP (Data Science Capstone JHU / Coursera)'
author: Paul Gomez
date: '2021-05-29'
slug: part-1-data-science-capstone-nlp-jhu-coursera
categories:
  - R
tags:
  - Data Science
  - NLP
  - R Markdown
  - Exploratory Analysis
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(message=FALSE)
```

## Introduction 

This series of post are dedicated to the final capstone of the [Data science specialization](https://www.coursera.org/specializations/jhu-data-science) of the John Hopkins University through coursera platform. The goal of this capstone is to understand and building predictive text models. As time pass I will be posting my steps to complete each assigment, here called tasks, until achieve the goal of having a fully functional model and a shiny app to run it online.

This first post is dedicated to:

- **Task 1** - Getting and cleaning the data
- **Task 2** - Exploratory Data Analysis

## Task 1 - Getting and Cleaning the Data

In this part, I will be using an English database. Tasks to accomplish are:

- **Obtaining the data** 
- **Tokenization** - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
- **Profanity filtering** - removing profanity and other words I do not want to predict.

### Getting the data

The data was supplied for the instructors, but we are allowed to expand the data set if we consider it's appropriate. As a first approach I will use just the data set supplied and depending of the results of this approach I may consider extend it in the future.

The data can be downloaded [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

```{r datapath, include=FALSE}
# files path
path <- list(blogs = "C:/Repositorios Github/JHU-specialization/JHU-DS-specialization/DATA_SCIENCE_CAPSTONE/data/sample.en_US.blogs.txt",
          news = "C:/Repositorios Github/JHU-specialization/JHU-DS-specialization/DATA_SCIENCE_CAPSTONE/data/sample.en_US.news.txt",
          twitter = "C:/Repositorios Github/JHU-specialization/JHU-DS-specialization/DATA_SCIENCE_CAPSTONE/data/sample.en_US.twitter.txt")
```
```{r reading data, message=FALSE, cache=FALSE}
library(tidyverse)

blogs <- read_tsv(path$blogs) %>% rename("text"="sample_lines.x..n...round.nl.20..")
news <- read_tsv(path$news) %>% rename("text"="sample_lines.x..n...round.nl.20..")
twitter <- read_tsv(path$twitter) %>% rename("text"="sample_lines.x..n...round.nl.20..")
rm(path)
```

A fast exploration of this files says blogs data has `r nrow(blogs)` lines, News has `r nrow(news)` lines and twitter `r nrow(blogs)` lines. 

Preview of blogs data set: 
```{r, message=FALSE, echo=FALSE}
head(blogs)
```

### Tokenization

Tokenization is a key (and mandatory) aspect of working with text data, Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.

Tokenization is performed on the corpus to obtain tokens. The following tokens are then used to prepare a vocabulary. Vocabulary refers to the set of unique tokens in the corpus. 

To perform an exploratory analysis, I'll tokenize the data in 1 word, 2 words and 3 words (1-gram, 2-grams, 3-grams). I will use the tidytext package for this task. First define a function to 

```{r tokenization, message=FALSE, cache=FALSE}
library(tidytext)

token_func <- function(rawdata){
  # Input : raw data. tibble n x 1. 1 sentence per row. 
  # Output : tokenized Version of sentences. list of tibbles.
  #           list[[1]] is a tibble m x 1 (1-gram)
  #           list[[2]] is a tibble m x 2 (2-gram)
  #           list[[3]] is a tibble m x 3 (3-gram)
  token_w4 <- rawdata %>% 
    unnest_tokens(word, text, token = "ngrams", n=3, to_lower = T)
  token_w4 <- token_w4 %>% separate(word, c("word1", "word2", "word3"), sep = " ")
    tokens_list <- list(token_w4[,1],token_w4[,1:2],token_w4[,1:3])
}

blogs_list <- token_func(blogs)  
news_list <- token_func(news)
twitter_list <- token_func(twitter)

token1 <- rbind(blogs_list[[1]], news_list[[1]], twitter_list[[1]])
token2 <- rbind(blogs_list[[2]], news_list[[2]], twitter_list[[2]])
token3 <- rbind(blogs_list[[3]], news_list[[3]], twitter_list[[3]])

head(token3)
```

```{r removing var 1, include=FALSE}
rm(blogs_list, news_list, twitter_list)
```

### Profanity filtering

In this step we want to remove profanity and other words we do not want to predict. I'll use the **lexicon** package as source for a profanity vocabulary. First step is to create a tibble with profan vacabulary:

```{r Profanity words, cache=FALSE, message=FALSE}
library(lexicon) 
profan_words <- tibble(profanity_alvarez) %>% rename("word"="profanity_alvarez")
```

Now, we need a function to filter our data. In addition to profanity filytering, I'm going to make some aditional filtering. The function first filter one character words, words that contain numbers and some special characters, finally remove profanity words. 

```{r filter function, message=FALSE}
separar_filtrar <- function(data, profan){
    if(ncol(data) == 3){
        ngrams_filtered <- data %>%
            filter(!(is.na(word1) | is.na(word2) | is.na(word3) )) %>% 
            filter(!((nchar(word1) == 1) | (nchar(word2) == 1) | (nchar(word3) == 1) ) ) %>% 
            filter(!(str_detect(word1, "[0-9]") | str_detect(word2, "[0-9]") | str_detect(word3, "[0-9]") ) ) %>% 
            filter(!(str_detect(word1, "â") | str_detect(word2, "â") | str_detect(word3, "â") ) ) %>%
            filter(!(word1 %in% profan$word | word2 %in% profan$word | word3 %in% profan$word)) %>% 
            filter(!(str_detect(word1, "hah") | str_detect(word2, "hah") | str_detect(word3, "hah") ) )
    }else if(ncol(data) == 2){
        ngrams_filtered <- data %>%
            filter(!(is.na(word1) | is.na(word2) )) %>% 
            filter(!((nchar(word1) == 1) | (nchar(word2) == 1) ) ) %>% 
            filter(!(str_detect(word1, "[0-9]") | str_detect(word2, "[0-9]") ) ) %>% 
            filter(!(str_detect(word1, "â") | str_detect(word2, "â") ) ) %>% 
            filter(!(word1 %in% profan$word | word2 %in% profan$word )) %>%
            filter(!(str_detect(word1, "hah") | str_detect(word2, "hah") ) )            
    }else if(ncol(data) == 1){
        ngrams_filtered <- data %>%
            filter(!(is.na(word1))) %>% 
            filter(!((nchar(word1) == 1) ) ) %>% 
            filter(!(str_detect(word1, "[0-9]") )) %>% 
            filter(!(str_detect(word1, "â") ) ) %>% 
            filter(!(word1 %in% profan$word )) %>%
            filter(!(str_detect(word1, "hah") ) )
    }
        ngrams_filtered
}
```

Applying the *separar_filtrar()* function to out data:

```{r applying separar_filtrar, cache = FALSE}
token1 <- separar_filtrar(token1, profan_words)
token2 <- separar_filtrar(token2, profan_words)
token3 <- separar_filtrar(token3, profan_words)
```
       
## Task 2 - Exploratory Data Analysis

A good first step to understand the vocabulary is to count frequency of tokens. Lets begin exploring the 1-word per token (1-gram) data:

```{r count 1-gram}
token1 <- token1 %>% count(word1, sort = TRUE)
occurrences <- sum(token1$n)
token1 <- token1 %>% mutate(freq = n/occurrences)

token1 %>% summary()
```

Here we can see that we have a really huge vocabulary, there are more than a hundred thousand words! But, in the other hand, the summary of this vocabulary shows that 75% of the words are repeated just 5 times or less. Cut the data by frecuency could help making our model faster and lighther. The most frequent words are:

```{r frequency plot 1 gram, echo=FALSE}
# plot with the 20 most frequent words in vocabulary
token1 %>% .[1:20,] %>% 
    mutate(name = fct_reorder(word1, n)) %>%
    ggplot( aes(x=name, y=n)) +
    geom_bar(stat="identity", fill="black", alpha=.6, width=.4) +
    coord_flip() + ylab("Counts") +
    xlab("") + ggtitle("20 Most Frequent Words") +
    theme_gray()
```

Now I will select just words with a frequency >3, we want to reduce the vocabulary size:  

```{r cutting down vocabulary}
# selecting words with count >3
token1 <- token1 %>% filter(n > 3) 
occurrences <- sum(token1$n)
token1 <- token1 %>% mutate(freq = n/occurrences)

token1 %>% summary()
```
```{r, echo=FALSE}
head(token1)
```

Now we have a vocabulary of `r nrow(news)` words. The next step is to filter the 2-gram and 3-grams tokens to keep just words which belongs to the vocabulary:

```{r cutting down 2 and 3 grams}
# this function gets 2-gram and 3-gram data (1 word per column) and 
# remove words that doesn't appear in the vocabulary 
clean_func <- function(dat, vocabulary){
    if(ncol(dat) == 2){
        dat %>% filter((word1 %in% vocabulary$word1) | (word2 %in% vocabulary$word1))
    }else if(ncol(dat) == 3){
        dat %>% filter((word1 %in% vocabulary$word1) | (word2 %in% vocabulary$word1)  | (word3 %in% vocabulary$word1))
    }
}

# Filtering token2 y token3
token2 <- clean_func(token2, token1)
token3 <- clean_func(token3, token1)
rm(blogs, news, twitter)

head(token3)
```
To count Bigrams and Trigrams, it's necesesary to combine each row in a single character column:  

```{r token2 and token3 counts}
token2 <- token2 %>% 
    unite(bigram, word1, word2, sep = " ") %>% 
    count(bigram, sort = T)

token2 %>% .[1:20,] %>% 
    mutate(name = fct_reorder(bigram, n)) %>%
    ggplot(aes(x=name, y=n)) +
    geom_bar(stat="identity", fill="black", alpha=.6, width=.4) +
    coord_flip() + 
    xlab("") + ylab("Counts") + ggtitle("20 Most Frequent Bigrams") +
    theme_gray()
```

Now we can see which are the most common trigrams in our data:

```{r frequency plot 3 gram, echo=FALSE}
token3 <- token3 %>% 
    unite(trigram, word1, word2, word3, sep = " ") %>% 
    count(trigram, sort = T)

token3 %>% .[1:20,] %>% 
    mutate(name = fct_reorder(trigram, n)) %>%
    ggplot(aes(x=name, y=n)) +
    geom_bar(stat="identity", fill="black", alpha=.6, width=.4) +
    coord_flip() + 
    xlab("") + ylab("Counts") + ggtitle("20 Most Frequent Trigrams") +
    theme_gray()
```

An interesting question is - How many unique words do we need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? Lets see:

```{r}
token1 %>% 
    ggplot(aes(x=1:nrow(token1), y=cumsum(freq))) + 
    geom_jitter() + xlab("Word rank") + 
    ylab("Freq Cumulative sum") +
    geom_hline(yintercept=0.95, color = "blue") +
    geom_hline(yintercept=0.75, color = "pink") +
    geom_hline(yintercept=0.50, color = "red") +
    theme_gray()
```
```{r, include=FALSE}
cum_sum <- cumsum(token1$freq) 
cum_050 <- sum(cum_sum <= 0.50)
cum_075 <- sum(cum_sum <= 0.75)
cum_095 <- sum(cum_sum <= 0.95)
```

In the graph above we can see we need `r cum_050` to explain 50% of occurrences, `r cum_075` to explain 75% of occurrences and `r cum_095` to explain 95% of occurrences in our data..